# scripts/coleta_dados/raspagem_web.py
# Este script realiza a coleta de dados das IES públicas de SP e PR utilizando web scraping.

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

def coletar_dados_ies(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Exemplo de extração de dados (ajuste conforme a estrutura real do site)
    tabela = soup.find('table', {'id': 'tabela-ies'})
    dados = []
    for linha in tabela.find_all('tr')[1:]:
        colunas = linha.find_all('td')
        dados.append({
            'nome': colunas[0].text.strip(),
            'endereco': colunas[1].text.strip(),
            'cidade': colunas[2].text.strip(),
            'estado': colunas[3].text.strip(),
            'telefone': colunas[4].text.strip(),
            'numero_alunos': int(colunas[5].text.strip()),
            'taxa_evasao': float(colunas[6].text.strip().replace('%', ''))
        })
    
    return pd.DataFrame(dados)

def main():
    urls = [
        'http://exemplo.gov.br/ies_sp',
        'http://exemplo.gov.br/ies_pr'
    ]
    
    dados_totais = pd.DataFrame()
    for url in urls:
        print(f'Coletando dados de {url}')
        dados = coletar_dados_ies(url)
        dados_totais = pd.concat([dados_totais, dados], ignore_index=True)
        time.sleep(2)  # Respeitar o servidor com uma pausa

    dados_totais.to_csv('dados/bruto/ies_dados_brutos.csv', index=False)
    print('Coleta de dados concluída e salva em dados/bruto/ies_dados_brutos.csv')

if __name__ == '__main__':
    main()